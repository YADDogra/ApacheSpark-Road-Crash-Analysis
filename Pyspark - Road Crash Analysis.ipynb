{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analyzing Road Crash Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Write the code to create a SparkContext object using SparkSession, which tells Spark how to access a cluster. To create a SparkSession you first need to build a SparkConf object that contains information about your application. Give an appropriate name for your application and run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create a SparkContext object using SparkSession\n",
    "\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "# app_name = \"Assignment 1\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Import all the “Units” csv files from 2015-2019 into a single RDD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a Units csv rdd\n",
    "units_rdd = sc.textFile('*_DATA_SA_Units.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Import all the “Crashes” csv files from 2015-2019 into a single RDD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a Crashes csv rdd\n",
    "crashes_rdd = sc.textFile('*_DATA_SA_Crash.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. For each Units and Crashes RDDs, remove the header rows and display the total\n",
    "count and first 10 records.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to remove the next line character\n",
    "units_rdd = units_rdd.map(lambda line: line.split(','))\n",
    "\n",
    "# filter out header\n",
    "header_unit = units_rdd.first()\n",
    "filt_units_rdd = units_rdd.filter(lambda row: row != header_unit)  \n",
    "\n",
    "print(\"#### For Units ####\")\n",
    "print(f\"Number of lines: {filt_units_rdd.count()}\")  # to display the total count\n",
    "filt_units_rdd.take(10) # to display first 10 records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove the next line character\n",
    "crashes_rdd = crashes_rdd.map(lambda line: line.split(','))\n",
    "\n",
    "# to remove the header\n",
    "header_crash = crashes_rdd.first()\n",
    "filt_crashes_rdd = crashes_rdd.filter(lambda row: row != header_crash) \n",
    "\n",
    "print(\"\\n\\n#### For Crashes ####\")\n",
    "print(f\"Number of lines: {filt_crashes_rdd.count()}\") # to display the total count\n",
    "filt_crashes_rdd.take(10) # to display first 10 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Data Partitioning in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print number of records in each partition\n",
    "\n",
    "from pyspark.rdd import RDD\n",
    "def print_partitions(data):\n",
    "    if isinstance(data, RDD):\n",
    "        numPartitions = data.getNumPartitions()\n",
    "        partitions = data.glom().collect()\n",
    "    else:\n",
    "        numPartitions = data.rdd.getNumPartitions()\n",
    "        partitions = data.rdd.glom().collect()\n",
    "    \n",
    "    print(f\"####### NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions):\n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:5\n",
      "Partitioner:None\n",
      "####### NUMBER OF PARTITIONS: 5\n",
      "Partition 0: 35861 records\n",
      "Partition 1: 28163 records\n",
      "Partition 2: 33084 records\n",
      "Partition 3: 27713 records\n",
      "Partition 4: 29033 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions:{}\".format(filt_units_rdd.getNumPartitions())) # to display number of partitions\n",
    "print(\"Partitioner:{}\".format(filt_units_rdd.partitioner))\n",
    "print_partitions(filt_units_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:5\n",
      "Partitioner:None\n",
      "####### NUMBER OF PARTITIONS: 5\n",
      "Partition 0: 12964 records\n",
      "Partition 1: 16775 records\n",
      "Partition 2: 13237 records\n",
      "Partition 3: 13599 records\n",
      "Partition 4: 15431 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions:{}\".format(filt_crashes_rdd.getNumPartitions())) # to display number of partitions\n",
    "print(\"Partitioner:{}\".format(filt_crashes_rdd.partitioner))\n",
    "print_partitions(filt_crashes_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default Spark is partitioning data according to **Random equal partitioning** method. As we can see the number of records in each partition has less variation and both the rdds have 5 partitions each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Create a Key Value Pair RDD with Lic State as the key and rest of the other columns as value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"SA\"',\n",
       "  ('\"2016-1-15/08/2019\"',\n",
       "   '\"01\"',\n",
       "   '0',\n",
       "   '\"SA\"',\n",
       "   '\"OMNIBUS\"',\n",
       "   '\"2011\"',\n",
       "   '\"North\"',\n",
       "   '\"Male\"',\n",
       "   '\"056\"',\n",
       "   '\"HR\"',\n",
       "   '\"Full\"',\n",
       "   '\"Not Towing\"',\n",
       "   '\"Straight Ahead\"',\n",
       "   '\"010\"',\n",
       "   '\"5121\"',\n",
       "   '',\n",
       "   '')),\n",
       " ('',\n",
       "  ('\"2016-1-15/08/2019\"',\n",
       "   '\"02\"',\n",
       "   '1',\n",
       "   '',\n",
       "   '\"Pedestrian on Road\"',\n",
       "   '',\n",
       "   '\"East\"',\n",
       "   '\"Male\"',\n",
       "   '\"072\"',\n",
       "   '',\n",
       "   '',\n",
       "   '',\n",
       "   '\"Walking on Road\"',\n",
       "   '',\n",
       "   '\"5084\"',\n",
       "   '',\n",
       "   ''))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to create Key Value Pair RDD from units rdd\n",
    "result_pair = filt_units_rdd.map(lambda x: (x[9], (x[0], x[1], x[2], x[3], x[4], x[5], x[6],\\\n",
    "                                            x[7], x[8], x[10], x[11], x[12], x[13], x[14], x[15], x[16], x[17])))\n",
    "                                            \n",
    "result_pair.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Write the code to implement this partitioning in RDD using appropriate partitioning functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using hash partitioning function to partition rdd\n",
    "def hash_function(key):\n",
    "    val = 0\n",
    "    if key == '\"SA\"':\n",
    "        val = 2\n",
    "    else:\n",
    "        val = 3\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing hash partitioning with our function\n",
    "num_partitions = 2\n",
    "hash_partition_rdd = result_pair.partitionBy(num_partitions, hash_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Write the code to print the number of records in each partition. What does it tell about the data skewness?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 2\n",
      "Partition 0: 109684 records\n",
      "Partition 1: 44170 records\n"
     ]
    }
   ],
   "source": [
    "# using the above created print_partitions function to get records in each partition\n",
    "print_partitions(hash_partition_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the nnumber of records in each partition we can identify that **Partition 0** that denotes records belonging to **SA** are highly greater than that for other states. This implies that our data is highly skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average age of male and female drivers separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering done to remove empty and non-essential records from the AGE column\n",
    "clean_rdd = filt_units_rdd.filter(lambda x: '\"XXX\"' not in x[8]).filter(lambda x: x[8] != '')\n",
    "\n",
    "# to get the desired columns of GENDER and AGE\n",
    "male_female_rdd = clean_rdd.map(lambda x: (x[7].replace('\"',''), int(x[8].replace('\"',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Male', 40.975960299920004)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to aggregate only the male records in gender column \n",
    "male_rdd = male_female_rdd.filter(lambda x: x[0] == 'Male')\n",
    "grouped_male = male_rdd.groupByKey().map(lambda x: (x[0], sum(x[1])/len(x[1])))\n",
    "grouped_male.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Female', 40.38729268862415)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to aggregate only the female records in gender column \n",
    "female_rdd = male_female_rdd.filter(lambda x: x[0] == 'Female')\n",
    "grouped_female = female_rdd.groupByKey().map(lambda x: (x[0], sum(x[1])/len(x[1])))\n",
    "grouped_female.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oldest and the newest vehicle year involved in the accident? Display the Registration State, Year and Unit type of the vehicle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"SA\"', '\"Station Wagon\"', 2019)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the required columns from the rdd\n",
    "vehicle_reqq = filt_units_rdd.filter(lambda x: (x[5]!='' and 'XXXX' not in x[5]))\\\n",
    "                            .map(lambda x: (x[3],x[4],int(x[5].replace('\"',''))))\n",
    "\n",
    "# to find the details of newest vehicle\n",
    "vehicle_reqq.max(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"VIC\"', '\"Motor Cycle\"', 1900)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find the details of oldest vehicle\n",
    "vehicle_reqq.min(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load all units and crash data into two separate dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load data in two dataframes\n",
    "units_df = spark.read.csv(\"*_DATA_SA_Units.csv\",header=True)\n",
    "crashes_df = spark.read.csv(\"*_DATA_SA_Crash.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display the schema of the final two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Unit No: string (nullable = true)\n",
      " |-- No Of Cas: string (nullable = true)\n",
      " |-- Veh Reg State: string (nullable = true)\n",
      " |-- Unit Type: string (nullable = true)\n",
      " |-- Veh Year: string (nullable = true)\n",
      " |-- Direction Of Travel: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Lic State: string (nullable = true)\n",
      " |-- Licence Class: string (nullable = true)\n",
      " |-- Licence Type: string (nullable = true)\n",
      " |-- Towing: string (nullable = true)\n",
      " |-- Unit Movement: string (nullable = true)\n",
      " |-- Number Occupants: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- Rollover: string (nullable = true)\n",
      " |-- Fire: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Stats Area: string (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- LGA Name: string (nullable = true)\n",
      " |-- Total Units: string (nullable = true)\n",
      " |-- Total Cas: string (nullable = true)\n",
      " |-- Total Fats: string (nullable = true)\n",
      " |-- Total SI: string (nullable = true)\n",
      " |-- Total MI: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Area Speed: string (nullable = true)\n",
      " |-- Position Type: string (nullable = true)\n",
      " |-- Horizontal Align: string (nullable = true)\n",
      " |-- Vertical Align: string (nullable = true)\n",
      " |-- Other Feat: string (nullable = true)\n",
      " |-- Road Surface: string (nullable = true)\n",
      " |-- Moisture Cond: string (nullable = true)\n",
      " |-- Weather Cond: string (nullable = true)\n",
      " |-- DayNight: string (nullable = true)\n",
      " |-- Crash Type: string (nullable = true)\n",
      " |-- Unit Resp: string (nullable = true)\n",
      " |-- Entity Code: string (nullable = true)\n",
      " |-- CSEF Severity: string (nullable = true)\n",
      " |-- Traffic Ctrls: string (nullable = true)\n",
      " |-- DUI Involved: string (nullable = true)\n",
      " |-- Drugs Involved: string (nullable = true)\n",
      " |-- ACCLOC_X: string (nullable = true)\n",
      " |-- ACCLOC_Y: string (nullable = true)\n",
      " |-- UNIQUE_LOC: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to display the schema\n",
    "units_df.printSchema()\n",
    "crashes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import necessary functionality\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|Stats Area|  Suburb|Postcode|        LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|     Day|    Time|Area Speed|Position Type|Horizontal Align|Vertical Align|          Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight|    Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2018-601-17/01/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          8|        4|         0|       2|       2|2018| January|  Sunday|09:12 pm|       050|  Not Divided|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|   Night|Hit Pedestrian|       01|Driver Rider|        3: SI|     No Control|        null|          null|1329806.36|1670224.76|13298061670225|\n",
      "|2017-1613-15/08/2019|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          2|        4|         0|       0|       4|2017|February|Saturday|04:00 pm|       050|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|       01|Driver Rider|        2: MI|Traffic Signals|        null|          null|1327951.24|1669556.92|13279511669557|\n",
      "|2017-12182-15/08/...|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          6|        5|         0|       1|       4|2017|December|Saturday|04:08 pm|       050|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Wet| Not Raining|Daylight|Hit Pedestrian|       01|Driver Rider|        3: SI|Traffic Signals|        null|          null| 1329016.2|1670995.07|13290161670995|\n",
      "|2019-10404-8/07/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          4|        6|         0|       0|       6|2019| October|  Monday|08:20 am|       060| Divided Road|   Straight road|         Level|Driveway or Entrance|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|       01|Driver Rider|        2: MI|     No Control|        null|          null|1327088.72|1670880.07|13270891670880|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to convert 'Total Cas' column to integer type\n",
    "crashes_df = crashes_df.withColumn(\"Total Cas\", crashes_df[\"Total Cas\"].cast(IntegerType()))\n",
    "\n",
    "# to show all crashes in Adelaide with casualities greater than 3\n",
    "crashes_df.where((col(\"Suburb\") == 'ADELAIDE') & (col(\"Total Cas\") > 3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Display 10 crash events with highest casualties.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|    Stats Area|         Suburb|Postcode|            LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|      Day|    Time|Area Speed|Position Type|    Horizontal Align|Vertical Align|    Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight| Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2017-288-15/08/2019|2 Metropolitan|     PARA HILLS|    5096|   CITY OF SALISBURY|          2|       11|         0|       1|      10|2017| January|Wednesday|01:13 pm|       060|   T-Junction|       Straight road| Crest of Hill|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|Right Angle|       01|Driver Rider|        3: SI|      Stop Sign|        null|          null| 1334428.9|1683032.96|13344291683033|\n",
      "|2016-3035-15/08/2019|2 Metropolitan|        HACKHAM|    5163| CITY OF ONKAPARINGA|          3|        9|         3|       5|       1|2016| January| Saturday|11:50 am|       080|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1320361.49|1645195.63|13203611645196|\n",
      "|2016-6630-15/08/2019|2 Metropolitan|  KANGAROO FLAT|    5118|LIGHT REGIONAL CO...|          3|        9|         0|       2|       7|2016|   April|Wednesday|09:00 pm|       100|  Not Divided|CURVED, VIEW OBSC...|         Level|Not Applicable|      Sealed|          Dry| Not Raining|   Night|    Head On|       01|Driver Rider|        3: SI|     No Control|        null|          null|1339316.32|1710314.92|13393161710315|\n",
      "|2019-11734-8/07/2020|2 Metropolitan|          STURT|    5047|CC MARION.       ...|          2|        9|         0|       1|       8|2019|November|   Sunday|07:25 pm|       060|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|       02|Driver Rider|        3: SI|Traffic Signals|        null|          null|1324428.84|1659884.95|13244291659885|\n",
      "|2016-14407-15/08/...|     3 Country|      STOCKWELL|    5355|THE BAROSSA COUNCIL.|          2|        8|         1|       6|       1|2016| October|   Sunday|03:46 pm|       100|  Not Divided|       Straight road| Crest of Hill|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|    Head On|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1373964.45|1723462.57|13739641723463|\n",
      "|2016-7073-15/08/2019|     3 Country|       MERRITON|    5523|PT.PIRIE CITY & D...|          2|        8|         4|       3|       1|2016|   April|   Sunday|12:35 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1293759.89|1840109.96|12937601840110|\n",
      "|2015-2823-21/08/2019|     3 Country|         HAWKER|    5434|THE FLINDERS RANG...|          1|        8|         0|       0|       8|2015|   March|   Monday|06:00 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|  Roll Over|       01|Driver Rider|        2: MI|     No Control|        null|          null|1315077.61|2022309.34|13150782022309|\n",
      "|2015-12591-21/08/...|     3 Country|        MALLALA|    5502|DC MALLALA.      ...|          2|        7|         0|       2|       5|2015| October|   Sunday|02:30 pm|       100|   Cross Road|       Straight road|         Level|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|Right Angle|       01|Driver Rider|        3: SI|  Give Way Sign|        null|          null|1325122.01|1724860.95|13251221724861|\n",
      "|2015-13713-21/08/...|2 Metropolitan|ELIZABETH GROVE|    5112|   CITY OF PLAYFORD.|          2|        7|         0|       0|       7|2015|November|   Friday|03:42 pm|       080|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|   Rear End|       01|Driver Rider|        2: MI|     No Control|        null|          null|1336118.68|1691385.65|13361191691386|\n",
      "|2015-6965-21/08/2019|     3 Country|       BEAUFORT|    5550|YORKE PENINSULA C...|          3|        7|         3|       4|       0|2015|    June|   Monday|11:13 am|       100|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|       09|       Other|     4: Fatal|     No Control|        null|          null|1287930.19|1761652.36|12879301761652|\n",
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to show top ten crashes\n",
    "crashes_df.orderBy(\"Total Cas\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the total number of fatalities for each crash type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert 'Total Fats' column to integer type\n",
    "crashes_df = crashes_df.withColumn(\"Total Fats\", crashes_df[\"Total Fats\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          Crash Type|Number_of_Fatalities|\n",
      "+--------------------+--------------------+\n",
      "|    Hit Fixed Object|                 152|\n",
      "|             Head On|                  86|\n",
      "|      Hit Pedestrian|                  70|\n",
      "|           Roll Over|                  57|\n",
      "|         Right Angle|                  45|\n",
      "|          Side Swipe|                  20|\n",
      "|          Right Turn|                  18|\n",
      "|            Rear End|                  16|\n",
      "|  Hit Parked Vehicle|                   9|\n",
      "|          Hit Animal|                   4|\n",
      "|  Hit Object on Road|                   2|\n",
      "|               Other|                   2|\n",
      "|Left Road - Out o...|                   1|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to group the different crash types and display their total fatalities\n",
    "crash_group_df = crashes_df.groupBy('Crash Type')\n",
    "crash_group_df.agg(sum('Total Fats').alias('Number_of_Fatalities')).orderBy(\"Number_of_Fatalities\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Find the total number of casualties for each suburb when the vehicle was driven by an unlicensed driver. You are required to display the name of the suburb and the total number of casualties.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|         Suburb|Number_of_Casualities|\n",
      "+---------------+---------------------+\n",
      "|       ADELAIDE|                   19|\n",
      "|      SALISBURY|                   18|\n",
      "|      DRY CREEK|                   18|\n",
      "| SALISBURY EAST|                   16|\n",
      "|       PROSPECT|                   14|\n",
      "| NORTH ADELAIDE|                   13|\n",
      "|        ENFIELD|                   12|\n",
      "|   ANDREWS FARM|                   12|\n",
      "|SALISBURY DOWNS|                   11|\n",
      "|   BEDFORD PARK|                   11|\n",
      "|SALISBURY SOUTH|                   11|\n",
      "|     INGLE FARM|                   11|\n",
      "|     MUNNO PARA|                   10|\n",
      "|         BURTON|                   10|\n",
      "|SALISBURY PLAIN|                   10|\n",
      "|   MOUNT BARKER|                   10|\n",
      "| ELIZABETH PARK|                   10|\n",
      "|  MORPHETT VALE|                   10|\n",
      "|   MAWSON LAKES|                   10|\n",
      "| ELIZABETH EAST|                    9|\n",
      "+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join the two dataframes\n",
    "joined_df = units_df.join(crashes_df, units_df.REPORT_ID==crashes_df.REPORT_ID,how='inner')\n",
    "\n",
    "# to display the casualities when the driver is unlicenced\n",
    "joined_df.select(\"Suburb\", \"Total Cas\")\\\n",
    "        .where((col(\"Licence Type\") == 'Unlicenced'))\\\n",
    "        .groupBy('Suburb').agg(sum('Total Cas').alias('Number_of_Casualities'))\\\n",
    "        .orderBy(\"Number_of_Casualities\", ascending=False)\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Find the total number of crash events for each severity level. Which severity level is the most common?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|CSEF Severity|Number_of_events|\n",
      "+-------------+----------------+\n",
      "|       1: PDO|           46696|\n",
      "|        2: MI|           21881|\n",
      "|        3: SI|            2978|\n",
      "|     4: Fatal|             451|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to find the total number of crash events for each severity level\n",
    "crashes_df.groupBy(\"CSEF Severity\").agg(count(\"*\").alias('Number_of_events'))\\\n",
    "          .orderBy('Number_of_events',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Compute the total number of crash events for each severity level and the percentage for the four different scenarios.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. When the driver is tested positive on drugs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the required conditions in dataframe\n",
    "crash_count_df = crashes_df.select('CSEF Severity')\\\n",
    "        .where(col(\"Drugs Involved\") == 'Y')\\\n",
    "        .groupBy(\"CSEF Severity\").agg(count(\"*\").alias('Count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|CSEF Severity|Count|Percentage|\n",
      "+-------------+-----+----------+\n",
      "|     4: Fatal|   82|    6.540%|\n",
      "|        2: MI|  749|   59.730%|\n",
      "|       1: PDO|  176|   14.040%|\n",
      "|        3: SI|  247|   19.700%|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# used the below links to get strings in required format\n",
    "#https://stackoverflow.com/a/43992110\n",
    "#https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/\n",
    "total = crash_count_df.select(\"Count\").agg({\"Count\": \"sum\"}).collect().pop()['sum(Count)']\n",
    "crash_count_df.withColumn('Percentage', format_string(\"%2.3f%%\", round((crash_count_df['Count']/total * 100),2))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. When the driver is tested positive for blood alcohol concentration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the required conditions in dataframe\n",
    "crash_alcohol_df = crashes_df.select('CSEF Severity')\\\n",
    "        .where(col('DUI Involved') != '')\\\n",
    "        .groupBy(\"CSEF Severity\").agg(count(\"*\").alias('Count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|CSEF Severity|Count|Percentage|\n",
      "+-------------+-----+----------+\n",
      "|     4: Fatal|   79|    3.510%|\n",
      "|        2: MI|  737|   32.780%|\n",
      "|       1: PDO| 1173|   52.180%|\n",
      "|        3: SI|  259|   11.520%|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to get the output in required format\n",
    "total = crash_alcohol_df.select(\"Count\").agg({\"Count\": \"sum\"}).collect().pop()['sum(Count)']\n",
    "crash_alcohol_df.withColumn('Percentage', format_string(\"%2.3f%%\", round((crash_alcohol_df['Count']/total * 100),2))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. When the driver is tested positive for both drugs and blood alcohol**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the required conditions in dataframe\n",
    "crash_both_df = crashes_df.select('CSEF Severity')\\\n",
    "        .where((col('DUI Involved') != '') & (col(\"Drugs Involved\") == 'Y'))\\\n",
    "        .groupBy(\"CSEF Severity\").agg(count(\"*\").alias('Count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|CSEF Severity|Count|Percentage|\n",
      "+-------------+-----+----------+\n",
      "|     4: Fatal|   27|   15.430%|\n",
      "|        2: MI|   89|   50.860%|\n",
      "|       1: PDO|   24|   13.710%|\n",
      "|        3: SI|   35|   20.000%|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to get the output in required format\n",
    "total = crash_both_df.select(\"Count\").agg({\"Count\": \"sum\"}).collect().pop()['sum(Count)']\n",
    "crash_both_df.withColumn('Percentage', format_string(\"%2.3f%%\", round((crash_both_df['Count']/total * 100),2))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. When the driver is tested negative for both (no alcohol and no drugs).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the required conditions in dataframe\n",
    "negative_crash_df = crashes_df.select('CSEF Severity')\\\n",
    "        .where((col('DUI Involved').isNull()) & (col(\"Drugs Involved\").isNull()))\\\n",
    "        .groupBy(\"CSEF Severity\").agg(count(\"*\").alias('Count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|CSEF Severity|Count|Percentage|\n",
      "+-------------+-----+----------+\n",
      "|     4: Fatal|  317|    0.460%|\n",
      "|        2: MI|20484|   29.830%|\n",
      "|       1: PDO|45371|   66.060%|\n",
      "|        3: SI| 2507|    3.650%|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to get the output in required format\n",
    "total = negative_crash_df.select(\"Count\").agg({\"Count\": \"sum\"}).collect().pop()['sum(Count)']\n",
    "negative_crash_df.withColumn('Percentage', format_string(\"%2.3f%%\", round((negative_crash_df['Count']/total * 100),2))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Find the Date and Time of Crash, Number of Casualties in each unit and the Gender, Age, License Type of the unit driver for the suburb \"Adelaide\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Spark RDD\n",
    "\n",
    "req_crashes_rdd = filt_crashes_rdd.map(lambda x: (x[0].replace('\"',''),(x[2].replace('\"',''),x[10].replace('\"',''),\\\n",
    "                              x[11].replace('\"',''),x[12].replace('\"',''),x[13].replace('\"',''),x[6].replace('\"',''))))\n",
    "\n",
    "req_unit_rdd = filt_units_rdd.map(lambda x: (x[0].replace('\"',''),(x[7].replace('\"',''), x[8].replace('\"',''),\\\n",
    "                                            x[11].replace('\"',''))))\n",
    "\n",
    "# join the rdd on key\n",
    "join_rdd = req_crashes_rdd.join(req_unit_rdd).filter(lambda x: x[1][0][0] == 'ADELAIDE')\n",
    "join_rdd.map(lambda x: (x[1][0][1] +\"-\"+ x[1][0][2] +\"-\"+ x[1][0][3], x[1][0][4], x[1][0][5],\\\n",
    "                        x[1][1][0], x[1][1][1],x[1][1][2])).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|                Date|    Time|Total Cas|    Sex| Age|Licence Type|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 056|        Full|\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 072|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1|   Male| 056|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1| Female| 027|        null|\n",
      "|2016-November-Tue...|05:00 pm|        0| Female| 032|        Full|\n",
      "|2016-November-Tue...|05:00 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 022|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 020|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|   Male| 042|        Full|\n",
      "|2016-November-Monday|11:26 pm|        0|   null|null|        null|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 026|     Unknown|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 038|        Full|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 036|        Full|\n",
      "|2016-November-Tue...|05:05 pm|        0|   Male| 025|     Unknown|\n",
      "|2016-November-Tue...|05:05 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|03:30 pm|        1| Female| 065|        Full|\n",
      "|2016-November-Wed...|03:30 pm|        1|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|02:20 pm|        0|   Male| 063|        Full|\n",
      "|2016-November-Wed...|02:20 pm|        0|   null|null|        null|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 15.9 ms, sys: 1.35 ms, total: 17.2 ms\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Spark DataFrame\n",
    "\n",
    "# join the two dataframes\n",
    "joined_df = units_df.join(crashes_df, units_df.REPORT_ID==crashes_df.REPORT_ID,how='inner')\n",
    "# used the below link to find out concat function\n",
    "#https://www.edureka.co/community/2280/concatenate-columns-in-apache-spark-dataframe\n",
    "joined_df.select(concat(\"Year\", lit(\"-\"), \"Month\", lit(\"-\"),\"Day\").alias(\"Date\"), \"Time\", \"Total Cas\", \"Sex\", \"Age\", \"Licence Type\")\\\n",
    "        .where(col(\"Suburb\")=='ADELAIDE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|                Date|    Time|Total Cas|    Sex| Age|Licence Type|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 056|        Full|\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 072|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1|   Male| 056|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1| Female| 027|        null|\n",
      "|2016-November-Tue...|05:00 pm|        0| Female| 032|        Full|\n",
      "|2016-November-Tue...|05:00 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 022|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 020|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|   Male| 042|        Full|\n",
      "|2016-November-Monday|11:26 pm|        0|   null|null|        null|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 026|     Unknown|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 038|        Full|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 036|        Full|\n",
      "|2016-November-Tue...|05:05 pm|        0|   Male| 025|     Unknown|\n",
      "|2016-November-Tue...|05:05 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|03:30 pm|        1| Female| 065|        Full|\n",
      "|2016-November-Wed...|03:30 pm|        1|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|02:20 pm|        0|   Male| 063|        Full|\n",
      "|2016-November-Wed...|02:20 pm|        0|   null|null|        null|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 869 µs, sys: 3.49 ms, total: 4.36 ms\n",
      "Wall time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Spark SQL\n",
    "# Create Views from Dataframes\n",
    "crashes_df.createOrReplaceTempView(\"sql_crashes\")\n",
    "units_df.createOrReplaceTempView(\"sql_units\")\n",
    "\n",
    "# perform the sql operations\n",
    "spark.sql('''\n",
    "  SELECT Year || '-' || Month||'-'|| Day AS Date, Time, `Total Cas`, Sex, Age, `Licence Type`\n",
    "  FROM sql_crashes d JOIN sql_units w ON d.REPORT_ID=w.REPORT_ID\n",
    "  WHERE d.Suburb == 'ADELAIDE'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the total number of casualties for each suburb when the vehicle was driven by an unlicensed driver. You are required to display the name of the suburb and the total number of casualties.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|         Suburb|Number_of_Casualities|\n",
      "+---------------+---------------------+\n",
      "|       ADELAIDE|                   19|\n",
      "|      SALISBURY|                   18|\n",
      "|      DRY CREEK|                   18|\n",
      "| SALISBURY EAST|                   16|\n",
      "|       PROSPECT|                   14|\n",
      "| NORTH ADELAIDE|                   13|\n",
      "|        ENFIELD|                   12|\n",
      "|   ANDREWS FARM|                   12|\n",
      "|     INGLE FARM|                   11|\n",
      "|   BEDFORD PARK|                   11|\n",
      "|SALISBURY SOUTH|                   11|\n",
      "|SALISBURY DOWNS|                   11|\n",
      "|SALISBURY PLAIN|                   10|\n",
      "|   MOUNT BARKER|                   10|\n",
      "|     MUNNO PARA|                   10|\n",
      "| ELIZABETH PARK|                   10|\n",
      "|         BURTON|                   10|\n",
      "|  MORPHETT VALE|                   10|\n",
      "|   MAWSON LAKES|                   10|\n",
      "| ELIZABETH EAST|                    9|\n",
      "+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 13.2 ms, sys: 183 µs, total: 13.4 ms\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Spark DataFrame\n",
    "\n",
    "# join the two dataframes\n",
    "joined_df = units_df.join(crashes_df, units_df.REPORT_ID==crashes_df.REPORT_ID,how='inner')\n",
    "\n",
    "joined_df.select(\"Suburb\", \"Total Cas\")\\\n",
    "        .where((col(\"Licence Type\") == 'Unlicenced'))\\\n",
    "        .groupBy('Suburb').agg(sum('Total Cas').alias('Number_of_Casualities'))\\\n",
    "        .orderBy(\"Number_of_Casualities\", ascending=False)\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|         Suburb|Number_of_Casualities|\n",
      "+---------------+---------------------+\n",
      "|       ADELAIDE|                   19|\n",
      "|      SALISBURY|                   18|\n",
      "|      DRY CREEK|                   18|\n",
      "| SALISBURY EAST|                   16|\n",
      "|       PROSPECT|                   14|\n",
      "| NORTH ADELAIDE|                   13|\n",
      "|        ENFIELD|                   12|\n",
      "|   ANDREWS FARM|                   12|\n",
      "|     INGLE FARM|                   11|\n",
      "|SALISBURY DOWNS|                   11|\n",
      "|SALISBURY SOUTH|                   11|\n",
      "|   BEDFORD PARK|                   11|\n",
      "|SALISBURY PLAIN|                   10|\n",
      "|     MUNNO PARA|                   10|\n",
      "|   MOUNT BARKER|                   10|\n",
      "| ELIZABETH PARK|                   10|\n",
      "|         BURTON|                   10|\n",
      "|  MORPHETT VALE|                   10|\n",
      "|   MAWSON LAKES|                   10|\n",
      "|ELIZABETH GROVE|                    9|\n",
      "+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 5.45 ms, sys: 0 ns, total: 5.45 ms\n",
      "Wall time: 5.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Spark SQL\n",
    "# Create Views from Dataframes\n",
    "crashes_df.createOrReplaceTempView(\"sql_crashes\")\n",
    "units_df.createOrReplaceTempView(\"sql_units\")\n",
    "\n",
    "spark.sql('''\n",
    "  SELECT Suburb, sum(`Total Cas`) AS Number_of_Casualities\n",
    "  FROM sql_crashes d JOIN sql_units w ON d.REPORT_ID=w.REPORT_ID\n",
    "  WHERE w.`Licence Type` == 'Unlicenced'\n",
    "  GROUP BY Suburb\n",
    "  ORDER BY Number_of_Casualities desc\n",
    "''').show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
